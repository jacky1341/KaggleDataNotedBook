# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12SOaKUPGBHnet4-By_LvcsfMKQHCyp4W
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm
from scipy import stats

df_train = pd.read_csv('train.csv')

df_train.columns

df_train['SalePrice'].describe()

sns.distplot(df_train['SalePrice']);

# å³°å€¼
print('skewness: %f' % df_train['SalePrice'].skew())

# åå€¼

print('kurtosis: %f' % df_train['SalePrice'].kurt())

#'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. 
from matplotlib.pyplot import ylim
var = 'GrLivArea'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
data.plot.scatter(x = var, y ='SalePrice', ylim=(0,800000));

# 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. 
var = 'TotalBsmtSF'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
data.plot.scatter(x=var, y= 'SalePrice', ylim=(0,800000));

# OverallQual å’Œ saleprice å‘ˆæ­£æ€åˆ†å¸ƒ
# é€‰æ‹©åˆ†ç±»å˜é‡åŽ»æµ‹è¯•å˜é‡é—´çš„å…³ç³»
var = 'OverallQual'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(8,6))
fig = sns.boxplot(x=var, y='SalePrice', data=data)
fig.axis(ymin=0, ymax=800000);

var = 'YearBuilt'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(20,8))
fig = sns.boxplot(x=var, y ='SalePrice', data=data)
fig.axis(ymin=0,ymax=800000);
plt.xticks(rotation=90);

"""In summary
Stories aside, we can conclude that:

'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.
'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.
We just analysed four variables, but there are many other that we should analyse. **The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).**

That said, let's separate the wheat from the chaff.
"""

corrmat = df_train.corr()
f, ax = plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=.8, square = True);

"""At first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'GarageX' variables. Both cases show how significant the correlation is between these variables."""

k =10
f, ax = plt.subplots(figsize=(12,6))
cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(df_train[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, 
                 fmt = '.2f', annot_kws={'size':10}, 
                 yticklabels = cols.values, 
                 xticklabels = cols.values)
plt.show()

"""According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:

* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!
* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).
* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').
* 'FullBath'?? Really? 
* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?
* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.

Let's proceed to the scatter plots.
"""

sns.set()
cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
sns.pairplot(df_train[cols], size=2.5)
plt.show()

total = df_train.isnull().sum().sort_values(ascending=False)

percent = (df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending=False)

missing_data = pd.concat([total, percent], axis = 1, keys=['Total', 'Percent'])

missing_data.head(20)

"""Let's analyse this to understand how to handle the missing data.

We'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.

In what concerns the remaining cases, we can see that 'GarageX' variables have the same number of missing data. I bet missing data refers to the same set of observations (although I will not check it; it's just 5% and we should not spend 20 ð‘–ð‘›5  problems). Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'GarageX' variables. The same logic applies to 'BsmtX' variables.

Regarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.

Finally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.

In summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.
"""

df_train= df_train.drop((missing_data[missing_data['Total']> 1]).index,1)

df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)

df_train.isnull().sum().max()

saleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis])

low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()[:10]]

high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]

print('outer lower of the distribution')
print(low_range)
print('outer higher of the distribution')
print(high_range)

var = 'GrLivArea'

data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)

data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));

df_train.sort_values(by='GrLivArea', ascending=False)[:2]

df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)

df_train = df_train.drop(df_train[df_train['Id'] == 524].index)

var = 'TotalBsmtSF'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)

data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));

#histogram and normal probability plot
sns.distplot(df_train['SalePrice'], fit=norm);

fig = plt.figure()

res = stats.probplot(df_train['SalePrice'], plot = plt)

# å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨logåŽ»å¤„ç†æ­£åæ€çš„é—®é¢˜

df_train['SalePrice'] = np.log(df_train['SalePrice'])

sns.distplot(df_train['SalePrice'], fit=norm)

fig = plt.figure()

res = stats.probplot(df_train['SalePrice'], plot = plt)

# TotalBsmtSFå‡ºçŽ°äº†0ï¼Œ0ä¸å¯ä»¥ä½¿ç”¨logå¤„ç†
distplot(df_train['TotalBsmtSF'], fit=norm);

fig = plt.figure()

stats.probplot(df_train['TotalBsmtSF'], plot=plt)

# åˆ›å»º HasBsmtåˆ—
df_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)

# å°†æ‰€æœ‰çš„HasBsmtåˆ—çš„å€¼å˜ä¸º0 
df_train['HasBsmt'] = 0

# å°†æ‰€æœ‰TotalBsmtSF å¤§äºŽ0çš„å€¼éƒ½è®¾ç½®ä¸º1 
df_train.loc[df_train['TotalBsmtSF']>0, 'HasBsmt'] =1

# å°†æ‰€æœ‰å¯¹äºŽHasBsmtå¤§äºŽ1çš„å€¼ï¼Œå¯¹TotalBsmtSFå±žæ€§è¿›è¡Œlogå˜åŒ–
df_train.loc[df_train['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])

# å¯¹äºŽTotalBsmtSFå¤§äºŽ0çš„å€¼åšæ­£å¤ªåˆ†å¸ƒå›¾åƒ
sns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);

fig = plt.figure()

stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'],plot=plt )

# æŸ¥çœ‹æ•°æ®çš„åŒå¼‚æ€§ï¼Œä»Žç¦»æ•£å›¾å¯ä»¥çœ‹åˆ°æ¯ä¸ªå˜é‡é—´çš„æ–¹å·®åŒå¼‚æ€§
plt.scatter(df_train['GrLivArea'], df_train['SalePrice'])

plt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);

df_train = pd.get_dummies(df_train)